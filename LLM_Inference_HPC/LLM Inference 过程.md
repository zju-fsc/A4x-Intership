https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
# LLM Inference 介绍
自注意力机制（Self-Attention）是 Transformer 的核心部分，它通过对输入序列中的每个 Token 计算与其他 Token 的关系（相关性），生成更加上下文感知的表示。以下是对自注意力机制的详细讲解，重点解释 **Q（查询向量）、K（键向量）、V（值向量）** 的意义、维度以及它们在公式中的作用。

---

## 自注意力机制的目标
自注意力机制的目标是：**让每个 Token 根据整个序列中的其他 Token 的信息，生成一个上下文增强的表示。**这意味着模型可以捕捉单词之间的依赖关系，无论它们在序列中的距离如何。

---

## Q、K、V 的意义

在自注意力机制中，每个输入 Token 通过线性变换生成 **查询向量 (Query, Q)**、**键向量 (Key, K)** 和 **值向量 (Value, V)**。

### 1. **查询向量 (Q)** 
- **作用**：查询向量用于表示当前 Token 的“需求”，即它希望从其他 Token 中获取什么信息。例如，句子中单词 `it` 可能需要与之前的主语（如 `cat` 或 `dog`）建立联系。
- **意义**：Q 是用于比较的“问题”或“查询”，它会被用来计算当前 Token 与其他 Token 的相关性。

---

### 2. **键向量 (K)**
- **作用**：键向量表示每个 Token 的“属性”或“标识”，即它能够提供的信息。例如，句中 `cat` 的键向量可能强调它是一个名词，与代词 `it` 有潜在的语义关联。
- **意义**：K 是与 Q 进行比较的“答案”或“标识”，它帮助其他 Token 判断自己是否相关。

---

### 3. **值向量 (V)**
- **作用**：值向量是每个 Token 实际携带的信息，当一个 Token 被判定与当前 Token 相关时，其值向量会被用来更新当前 Token 的表示。例如，`cat` 的值向量可能包含上下文中有关 `cat` 的语义信息。
- **意义**：V 是返回的“内容”或“值”，它最终会被加权求和来生成输出。

---

总结：
- **Q** 问：我要什么信息？  
- **K** 答：我是什么信息？  
- **V** 给：具体的内容。

---

## Q、K、V 的维度

假设输入序列长度为 **n**，每个 Token 的嵌入维度为 **d_model**，则 Q、K、V 的维度如下：

1. 输入的嵌入向量（序列表示）：  
   - 输入序列$X$的维度为$(n, d_{model})$。
   
2. Q、K、V 的生成：  
   - Q、K、V 是通过线性变换从输入序列$X$中生成的：
     -$Q = XW_Q$ 
     -$K = XW_K$ 
     -$V = XW_V$ 
   - 其中$W_Q, W_K, W_V$是可训练的权重矩阵，维度分别为$(d_{model}, d_k)$、$(d_{model}, d_k)$、$(d_{model}, d_v)$。
   - 通常，**$d_k = d_v = d_{model}/h$**，其中$h$是注意力头的数量。

3. Q、K、V 的结果维度：  
   -$Q$:$(n, d_k)$ 
   -$K$:$(n, d_k)$ 
   -$V$:$(n, d_v)$

---

## 自注意力机制的计算公式

核心公式是：

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

### 各部分含义：
1. **$QK^T$**：
   -$QK^T$是查询向量$Q$与键向量$K$的点积结果。
   - 维度：$(n, d_k) \cdot (d_k, n) = (n, n)$。
   - 它表示序列中每个 Token 与其他所有 Token 的相关性分数（注意力分数）。

2. **$\frac{1}{\sqrt{d_k}}$**：
   - 对$QK^T$进行缩放，防止点积结果的数值过大导致梯度消失或 Softmax 输出过于极端。
   -$\sqrt{d_k}$是一个归一化因子。

3. **Softmax**：
   - 对每一行（即每个 Token）计算 Softmax，将注意力分数转换为概率分布。
   - 输出维度：$(n, n)$。
   - 表示每个 Token 对其他 Token 的注意力权重。

4. **加权求和$V$**：
   - 用注意力权重矩阵$\text{Softmax}(QK^T/\sqrt{d_k})$乘以值向量$V$，加权求和得到输出。
   - 维度：$(n, n) \cdot (n, d_v) = (n, d_v)$。
   - 最终输出是每个 Token 在上下文中的新的表示。

---

## 公式中的 Output 是什么？

最终的输出是每个 Token 的上下文表示，包含了它与其他 Token 的相关信息。

- **维度**：$(n, d_v)$，即序列长度$n$和值向量维度$d_v$。
- **意义**：每个 Token 的表示被增强为一个综合了上下文的向量，捕捉了与其他 Token 的关系和语义。

例如，在翻译任务中，某个单词的输出可能结合了句子中的主语、时态等上下文信息，更好地用于生成目标语言。

---

## 为什么叫做查询、键和值？

这些名称（Query、Key、Value）来源于信息检索中的概念：

1. **查询 (Query)**：用于查询某些信息。
2. **键 (Key)**：表示存储的信息的标识或索引。
3. **值 (Value)**：存储的信息内容。

在自注意力机制中，查询$Q$用于匹配键$K$，找到与其相关的值$V$。

---

## 总结

自注意力机制通过 **Q（查询）、K（键）、V（值）** 的交互，捕获输入序列中每个 Token 与其他 Token 的关系，生成上下文感知的表示。核心公式：

$$
\text{Output} = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

- **Q（查询向量）**：表示当前 Token 的需求。
- **K（键向量）**：表示其他 Token 的属性或标识。
- **V（值向量）**：表示其他 Token 的内容。
- **最终输出**：上下文增强的序列表示，维度为$(n, d_v)$。


# 具体过程
### **举例说明：使用"你是一只猫"的自注意力机制计算过程**

#### **输入句子：**
"你 是 一 只 猫"  
我们使用 **自注意力机制（Self-Attention）** 来处理这句话的特征表示，以捕获每个词与其他词的上下文依赖关系。

---

### **1. 准备输入**

#### **1.1 输入表示**
假设输入序列有 5 个词（"你"、"是"、"一"、"只"、"猫"），每个词被表示为一个固定维度的嵌入向量（Embedding）。假设嵌入的维度为$d = 4$。  
输入序列可以表示为矩阵$X$：
$$
X = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5
\end{bmatrix}
\in \mathbb{R}^{5 \times 4}
$$
其中$x_1, x_2, x_3, x_4, x_5$是五个词的嵌入向量，形状为$\mathbb{R}^4$。

假设：
$$
X = 
\begin{bmatrix}
1 & 0 & 1 & 0 \\  % “你”
0 & 1 & 1 & 0 \\  % “是”
1 & 1 & 0 & 1 \\  % “一”
0 & 1 & 0 & 1 \\  % “只”
1 & 0 & 0 & 1     % “猫”
\end{bmatrix}
$$

---

### **2. 线性变换生成 Query、Key 和 Value**

#### **2.1 权重矩阵**
自注意力机制需要生成三个向量：Query (Q)、Key (K)、Value (V)。这通过对输入$X$应用三个线性变换实现。

定义三个可学习的权重矩阵：
$$
W_Q, W_K, W_V \in \mathbb{R}^{4 \times 4}
$$

假设权重矩阵的值为：
$$
W_Q = 
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1
\end{bmatrix}, \quad
W_K = 
\begin{bmatrix}
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}, \quad
W_V = 
\begin{bmatrix}
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1
\end{bmatrix}
$$

#### **2.2 计算 Q、K、V**
通过矩阵乘法计算$Q, K, V$：

- **Query (Q)：**
$$
Q = X W_Q = 
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
1 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
2 & 0 & 2 & 0 \\
1 & 1 & 1 & 1 \\
2 & 2 & 2 & 2 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}
$$

- **Key (K)：**
$$
K = X W_K = 
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
1 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 2 & 0 & 2 \\
1 & 1 & 1 & 1 \\
2 & 2 & 2 & 2 \\
1 & 0 & 1 & 0 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

- **Value (V)：**
$$
V = X W_V = 
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
1 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1
\end{bmatrix}
=
\begin{bmatrix}
2 & 2 & 0 & 0 \\
1 & 1 & 1 & 1 \\
2 & 2 & 1 & 1 \\
0 & 0 & 1 & 1 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

---

### **3. 计算注意力分数**

#### **3.1 Query 与 Key 的点积**
对$Q$和$K$进行点积，计算注意力分数矩阵（未归一化）：
$$
QK^T =
\begin{bmatrix}
2 & 0 & 2 & 0 \\
1 & 1 & 1 & 1 \\
2 & 2 & 2 & 2 \\
0 & 1 & 0 & 1 \\
1 & 0 & 1 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & 1 & 2 & 1 & 1 \\
2 & 1 & 2 & 0 & 1 \\
0 & 1 & 2 & 1 & 1 \\
2 & 1 & 2 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
4 & 4 & 8 & 4 & 4 \\
6 & 4 & 8 & 2 & 4 \\
8 & 8 & 16 & 8 & 8 \\
2 & 2 & 4 & 2 & 2 \\
2 & 2 & 4 & 2 & 2
\end{bmatrix}
$$

#### **3.2 归一化分数**
通过缩放（除以$\sqrt{d} = \sqrt{4} = 2$）和 Softmax 归一化，得到注意力权重矩阵：
$$
\text{Attention Weights} = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)
$$

以第一个词（"你"）对应的权重计算为例：
$$
\frac{1}{2} \cdot [4, 4, 8, 4, 4] = [2, 2, 4, 2, 2]
$$
Softmax 计算后：
$$
\text{softmax}([2, 2, 4, 2, 2]) = [0.1, 0.1, 0.6, 0.1, 0.1]
$$

完整的注意力权重矩阵为：
$$
\text{Attention Weights} =
\begin{bmatrix}
0.1 & 0.1 & 0.6 & 0.1 & 0.1 \\
0.2 & 0.2 & 0.4 & 0.1 & 0.1 \\
0.2 & 0.2 & 0.4 & 0.1 & 0.1 \\
0.2 & 0.2 & 0.4 & 0.1 & 0.1 \\
0.2 & 0.2 & 0.4 & 0.1 & 0.1
\end{bmatrix}
$$

---

### **4. 加权求和**

将注意力权重与$V$相乘，得到每个词的最终输出表示：

以第一个词（"你"）为例：
$$
\text{Output}_1 = 0.1 \cdot V_1 + 0.1 \cdot V_2 + 0.6 \cdot V_3 + 0.1 \cdot V_4 + 0.1 \cdot V_5
$$

最终输出矩阵为：
$$
\text{Output} =
\begin{bmatrix}
1.6 & 1.6 & 0.6 & 0.6 \\
1.5 & 1.5 & 0.7 & 0.7 \\
1.6 & 1.6 & 0.6 & 0.6 \\
1.2 & 1.2 & 0.8 & 0.8 \\
1.2 & 1.2 & 0.8 & 0.8
\end{bmatrix}
$$

---

### **总结**

- **输入嵌入：** "你 是 一 只 猫" 被表示为向量。
- **计算 Query、Key 和 Value：** 通过线性变换生成$Q, K, V$。
- **注意力分数：** 通过点积和 Softmax 计算每个词对其他词的注意力权重。
- **加权求和：** 使用注意力权重对$V$进行加权求和得到输出。

输出矩阵$\text{Output}$是整句话的上下文表示，每个词的表示包含了与其他词的依赖关系。


