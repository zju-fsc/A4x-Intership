# 用左右手的关键点位置来确定是哪只手
```python
import os  
import os.path as osp  
import sys  
import time  
from omegaconf import OmegaConf  
from easydict import EasyDict as edict  
  
import torch  
from torch import nn  
from torch.utils.data import DataLoader  
from torch.optim import Adam  
  
from lib.models.mano import build_mano_aa  
from lib.utils.demo_utils import get_object_hand_info  
from lib.utils.model_utils import (  
    build_refiner,  
    build_model_and_diffusion,  
    build_seq_cvae,  
    build_mpnet,  
    build_pointnetfeat,  
    build_contact_estimator,  
)  
from lib.models.object import build_object_model  
from lib.networks.clip import load_and_freeze_clip, encoded_text  
from lib.utils.proc import proc_obj_feat_final, proc_refiner_input  
  
  
# 定义训练函数  
def train_epoch(  
        texthom, diffusion, seq_cvae, refiner, contact_estimator,  
        dataloader, optimizer, loss_fn, lhand_layer, rhand_layer, device, config  
):  
    texthom.train()  
    diffusion.train()  
    seq_cvae.train()  
    refiner.train()  
    contact_estimator.train()  
  
    total_loss = 0  
    for batch_idx, batch in enumerate(dataloader):  
        # 提取批次数据  
        text = batch["text"]  
        obj_pc = batch["obj_pc"]  
        obj_pc_normal = batch["obj_pc_normal"]  
        normalized_obj_pc = batch["normalized_obj_pc"]  
        obj_cent = batch["obj_cent"]  
        obj_scale = batch["obj_scale"]  
        is_lhand = batch["is_lhand"]  
        is_rhand = batch["is_rhand"]  
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  
  
        # 1. 文本编码  
        enc_text = encoded_text(clip_model, text).to(device)  
        obj_feat = pointnet(normalized_obj_pc.to(device))  
  
        # 2. 计算目标分布  
        duration = seq_cvae(enc_text)  
        duration *= 150  
        duration = duration.long().to(device)  
  
        valid_mask_lhand, valid_mask_rhand, valid_mask_obj = get_valid_mask_bunch(  
            is_lhand, is_rhand, config.dataset.max_nframes, duration  
        )  
  
        obj_feat_final, est_contact_map = proc_obj_feat_final(  
            contact_estimator, obj_scale, obj_cent, obj_feat, enc_text,  
            normalized_obj_pc.shape[1], config.texthom.use_obj_scale_centroid,  
            config.contact.use_scale, config.texthom.use_contact_feat  
        )  
  
        # 3. Diffusion 模型的采样  
        coarse_x_lhand, coarse_x_rhand, coarse_x_obj = diffusion.sampling(  
            texthom, obj_feat_final, enc_text, config.dataset.max_nframes,  
            config.texthom.hand_nfeats, config.texthom.obj_nfeats,  
            valid_mask_lhand, valid_mask_rhand, valid_mask_obj, device=device  
        )  
  
        # 4. Refinement 阶段  
        input_lhand, input_rhand, refined_x_obj = proc_refiner_input(  
            coarse_x_lhand, coarse_x_rhand, coarse_x_obj, lhand_layer, rhand_layer,  
            obj_pc, obj_pc_normal, valid_mask_lhand, valid_mask_rhand, valid_mask_obj,  
            est_contact_map, dataset_name=config.dataset.name  
        )  
  
        refined_x_lhand, refined_x_rhand = refiner(  
            input_lhand, input_rhand,  
            valid_mask_lhand=valid_mask_lhand,  
            valid_mask_rhand=valid_mask_rhand,  
        )  
  
        # 5. 计算损失  
        loss = loss_fn(refined_x_lhand, refined_x_rhand, refined_x_obj, obj_pc)  
        total_loss += loss.item()  
  
        # 6. 反向传播和优化  
        optimizer.zero_grad()  
        loss.backward()  
        optimizer.step()  
  
        if batch_idx % config.logging.log_interval == 0:  
            print(f"Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item():.4f}")  
  
    return total_loss / len(dataloader)  
  
  
def validate_epoch(dataloader, texthom, diffusion, seq_cvae, refiner, contact_estimator, loss_fn, config):  
    texthom.eval()  
    diffusion.eval()  
    seq_cvae.eval()  
    refiner.eval()  
    contact_estimator.eval()  
  
    total_loss = 0  
    with torch.no_grad():  
        for batch in dataloader:  
            # 与训练类似的流程，但不进行优化  
            # 计算损失并累计  
            pass  
  
    return total_loss / len(dataloader)  
  
def get_least_used_gpu():  
    """  
    获取显存占用最少的 GPU ID    """    min_memory = float('inf')  
    selected_gpu = -1  
  
    for gpu_id in range(torch.cuda.device_count()):  
        # 查询每个 GPU 的已分配显存和已保留显存  
        allocated_memory = torch.cuda.memory_allocated(gpu_id)  # 已使用显存  
        reserved_memory = torch.cuda.memory_reserved(gpu_id)   # 已预留显存  
        total_memory = allocated_memory + reserved_memory  
  
        # 找到显存占用最少的 GPU        if total_memory < min_memory:  
            min_memory = total_memory  
            selected_gpu = gpu_id  
  
    return selected_gpu  
  
  
def main_training_loop(config):  
    # 配置设备  
    # 获取显存占用最少的 GPU    least_used_gpu = get_least_used_gpu()  
  
    if least_used_gpu == -1:  
        print("No available GPUs found. Using CPU.")  
        device = torch.device("cpu")  
    else:  
        print(f"Using GPU: {least_used_gpu}")  
        device = torch.device(f"cuda:{least_used_gpu}")  
  
    # 构建模型  
    lhand_layer = build_mano_aa(is_rhand=False, flat_hand=config.dataset.flat_hand).to(device)  
    rhand_layer = build_mano_aa(is_rhand=True, flat_hand=config.dataset.flat_hand).to(device)  
    texthom, diffusion = build_model_and_diffusion(config, lhand_layer, rhand_layer).to(device)  
    seq_cvae = build_seq_cvae(config).to(device)  
    refiner = build_refiner(config).to(device)  
    contact_estimator = build_contact_estimator(config).to(device)  
    clip_model = load_and_freeze_clip(config.clip.clip_version).to(device)  
    pointnet = build_pointnetfeat(config).to(device)  
  
    # 加载数据  
    object_model = build_object_model(config.dataset.data_obj_pc_path)  
    train_dataset = ...  # 自定义训练数据集  
    val_dataset = ...  # 自定义验证数据集  
    train_loader = DataLoader(train_dataset, batch_size=config.train.batch_size, shuffle=True)  
    val_loader = DataLoader(val_dataset, batch_size=config.train.batch_size)  
  
    # 优化器和损失函数  
    optimizer = Adam([  
        {"params": texthom.parameters()},  
        {"params": diffusion.parameters()},  
        {"params": seq_cvae.parameters()},  
        {"params": refiner.parameters()},  
        {"params": contact_estimator.parameters()},  
    ], lr=config.train.lr)  
    loss_fn = nn.MSELoss()  # 示例：可以根据任务选择更合适的损失函数  
  
    # 训练循环  
    for epoch in range(config.train.epochs):  
        print(f"Epoch {epoch + 1}/{config.train.epochs}")  
        train_loss = train_epoch(  
            texthom, diffusion, seq_cvae, refiner, contact_estimator,  
            train_loader, optimizer, loss_fn, lhand_layer, rhand_layer, device, config  
        )  
        val_loss = validate_epoch(  
            val_loader, texthom, diffusion, seq_cvae, refiner, contact_estimator, loss_fn, config  
        )  
  
        print(f"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}")  
  
        # 保存模型  
        save_path = osp.join(config.checkpoints.dir, f"epoch_{epoch + 1}.pth")  
        torch.save({  
            "texthom": texthom.state_dict(),  
            "diffusion": diffusion.state_dict(),  
            "seq_cvae": seq_cvae.state_dict(),  
            "refiner": refiner.state_dict(),  
            "contact_estimator": contact_estimator.state_dict(),  
        }, save_path)  
        print(f"Model saved to {save_path}")  
  
  
if __name__ == "__main__":  
    from hydra import initialize, compose  
  
    initialize(config_path="../configs", version_base=None)  
    cfg = compose(config_name="config")  
    main_training_loop(cfg)
```
问题：
左右手识别问题
物体的位姿有问题，卡布奇诺盒子里面的小卡布奇诺没有输入，经常出现实际上用了两只手但是只显示一只手的效果，而且从盒子里面取东西，取东西的那只手判定为没有使用，因为小东西貌似没有被建模。


# 考虑帧之间的连续关系
1. 首先考虑前后帧之间同一只手的关节如果移动超过阈值就当做这只手参与
2. 还有考虑到前后动作的连贯性，如果上一帧动用了左手那么我们认为这一帧动用左手是很合理，所以可以减少1中的阈值
3. 同时保留靠近物体的判据
```python
import os
import numpy as np

class H2OTextAnnotation:
    def __init__(self, root_dir, output_file="annotations.txt", distance_threshold=0.02):
        self.root_dir = root_dir
        self.output_file = output_file
        self.distance_threshold = distance_threshold

        # 获取所有文件名
        self.file_names = sorted(os.listdir(os.path.join(root_dir, "action_label")))

        # 动作类别（动词）
        self.verb_labels = [
            "background", "grab", "place", "open", "close", "pour",
            "take out", "put in", "apply", "read", "spray", "squeeze"
        ]

        # 物体类别（名词）
        self.object_labels = [
            "background", "book", "espresso", "lotion", "spray",
            "milk", "cocoa", "chips", "cappuccino"
        ]

    def generate_annotations(self):
        annotations = []
        prev_interacting_hand = None  # 上一帧的交互手

        for idx, file_name in enumerate(self.file_names):
            # 构造每种数据的路径
            if idx > 0:
                pre_file_name = self.file_names[idx - 1]
                pre_hand_pose_path = os.path.join(self.root_dir, "hand_pose", pre_file_name)
                pre_hand_pose = self._read_txt(pre_hand_pose_path)
            else:
                pre_file_name = None
                pre_hand_pose = None

            action_label_path = os.path.join(self.root_dir, "action_label", file_name)
            verb_label_path = os.path.join(self.root_dir, "verb_label", file_name)
            obj_pose_path = os.path.join(self.root_dir, "obj_pose", file_name)
            hand_pose_path = os.path.join(self.root_dir, "hand_pose", file_name)

            # 读取数据
            action_label = self._read_txt(action_label_path, single_value=True)
            verb_label = self._read_txt(verb_label_path, single_value=True)
            obj_pose = self._read_txt(obj_pose_path)
            hand_pose = self._read_txt(hand_pose_path)

            # 获取物体类别和动词类别
            object_class = int(obj_pose[0])  # obj_pose 的第一个值是物体类别
            verb_class = int(verb_label)  # verb_label 是一个整数，表示动词类别

            # 确保类别有效
            if object_class < 0 or object_class >= len(self.object_labels):
                continue
            if verb_class < 0 or verb_class >= len(self.verb_labels):
                continue

            object_label = self.object_labels[object_class]
            verb_label = self.verb_labels[verb_class]

            # 解析物体关键点位置
            object_positions = obj_pose[1:].reshape(21, 3)  # 提取物体的 21 个关键点位置

            # 判断交互手
            interacting_hand = self._determine_interacting_hand(
                hand_pose,
                object_positions,
                pre_hand_pose,
                prev_interacting_hand
            )

            # 生成文本描述
            if interacting_hand is not None:
                annotation = f"{verb_label.capitalize()} the {object_label} with {interacting_hand}."
            else:
                annotation = f'{verb_label.capitalize()} the {object_label} without hands.'
            annotations.append(annotation)

            # 更新上一帧的交互手
            prev_interacting_hand = interacting_hand

        # 保存文本标注到文件
        with open(self.output_file, "w") as f:
            for annotation in annotations:
                f.write(annotation + "\n")

        print(f"Annotations saved to {self.output_file}")

    def _determine_interacting_hand(self, hand_pose, object_positions, prev_hand_pose=None, prev_interacting_hand=None,
                                    default_movement_threshold=0.2, reduced_movement_threshold=0.01):
        """
        判断哪只手与物体交互，基于以下三个条件：
        1. 所有关节到物体关键点的最小距离。
        2. 当前帧手关节位置与上一帧位置的差异（运动变化）。
        3. 上一帧的交互手（动态调整运动阈值）。

        Args:
            hand_pose (np.array): 当前帧的左右手所有关节位置 (128,)。
            object_positions (np.array): 物体的所有关键点位置 (21, 3)。
            prev_hand_pose (np.array): 上一帧左右手所有关节位置 (128,)。如果为 None，则忽略运动变化判据。
            prev_interacting_hand (str): 上一帧的交互手（"left hand"、"right hand"、"both hands" 或 None）。
            default_movement_threshold (float): 默认判定运动变化的阈值（单位：米）。
            reduced_movement_threshold (float): 动态调整的较低运动阈值（单位：米）。

        Returns:
            str: 返回交互手的类型（"left hand"、"right hand"、"both hands" 或 None）。
        """
        # 提取左手和右手关节位置
        left_hand_joints = hand_pose[1:64].reshape(21, 3)  # 左手21个关节
        right_hand_joints = hand_pose[65:128].reshape(21, 3)  # 右手21个关节

        # 物体交互判据：计算左手所有关节到物体所有关键点的最小距离
        left_hand_dist = np.min([
            np.linalg.norm(left_joint - obj_point)
            for left_joint in left_hand_joints
            for obj_point in object_positions
        ])
        right_hand_dist = np.min([
            np.linalg.norm(right_joint - obj_point)
            for right_joint in right_hand_joints
            for obj_point in object_positions
        ])

        # 初始化交互标志
        left_hand_interact = left_hand_dist < self.distance_threshold
        right_hand_interact = right_hand_dist < self.distance_threshold

        # 如果提供了上一帧的手关节位置，增加运动变化判据
        if prev_hand_pose is not None:
            # 提取上一帧的左手和右手关节位置
            prev_left_hand_joints = prev_hand_pose[1:64].reshape(21, 3)
            prev_right_hand_joints = prev_hand_pose[65:128].reshape(21, 3)

            # 动态调整运动阈值
            left_movement_threshold = reduced_movement_threshold if prev_interacting_hand in ["left hand",
                                                                                              "both hands"] else default_movement_threshold
            right_movement_threshold = reduced_movement_threshold if prev_interacting_hand in ["right hand",
                                                                                               "both hands"] else default_movement_threshold

            # 计算左手和右手关节的运动变化（关节位置的变化距离之和）
            left_hand_movement = np.sum(np.linalg.norm(left_hand_joints - prev_left_hand_joints, axis=1))
            right_hand_movement = np.sum(np.linalg.norm(right_hand_joints - prev_right_hand_joints, axis=1))

            # 如果运动变化超过阈值，也将该手判定为交互
            if left_hand_movement > left_movement_threshold:
                left_hand_interact = True
            if right_hand_movement > right_movement_threshold:
                right_hand_interact = True

        # 根据交互标志返回结果
        if left_hand_interact and right_hand_interact:
            return "both hands"
        elif left_hand_interact:
            return "left hand"
        elif right_hand_interact:
            return "right hand"
        else:
            return None

    def _read_txt(self, file_path, single_value=False):
        """
        读取 .txt 文件，支持多行多列数据。

        Args:
            file_path (str): 文件路径。
            single_value (bool): 如果为 True，返回单个数值。

        Returns:
            np.array 或 float: 读取的数组或单个数值。
        """
        with open(file_path, "r") as f:
            data = np.loadtxt(f, dtype=np.float32)
        if single_value:
            return data.item()  # 转换为单个数值
        return data


# 使用示例
dataset_root = "./datasets/h2o/subject1/h1/0/cam4"

# 创建生成器对象
text_annotation_generator = H2OTextAnnotation(root_dir=dataset_root)

# 生成文本标注并保存到文件
text_annotation_generator.generate_annotations()
```

